import streamlit as st
import feedparser
import requests
from bs4 import BeautifulSoup
from feedgen.feed import FeedGenerator # ì •í™•í•œ ì„í¬íŠ¸ ê²½ë¡œ
import datetime
import os

# --- Page Configuration ---
st.set_page_config(
    page_title="ì…œë¡ ì•„í‹°í´ í”¼ë“œ (ë™ì  ìƒì„±)",
    page_icon="ğŸ“°",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# --- Constants for RSS Generation ---
RSS_ARCHIVES_URL = "https://www.neosherlock.com/archives"
# ìƒì„±ë  RSS íŒŒì¼ ì´ë¦„ì´ì, Streamlit ì•±ì´ ì½ì„ íŒŒì¼ ì´ë¦„
RSS_FILE_PATH = "neosherlock_feed_final.xml"
RSS_USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'

# --- RSS Generation Functions ---
def fetch_html_for_rss(url):
    headers = {'User-Agent': RSS_USER_AGENT}
    try:
        response = requests.get(url, headers=headers, timeout=15) # íƒ€ì„ì•„ì›ƒ ì•½ê°„ ëŠ˜ë¦¼
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"ğŸš« RSS Gen: URL ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {url} - {e}") # ì„œë²„ ë¡œê·¸ìš©
        return None

def parse_articles_for_rss(html_content):
    """HTML ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ ê¸°ì‚¬ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì‚¬ìš©ì ì œê³µ HTML ê¸°ë°˜ìœ¼ë¡œ ê²€ì¦ëœ ë¡œì§)"""
    if not html_content:
        return []
    soup = BeautifulSoup(html_content, 'html.parser')
    articles_data = []
    main_content = soup.find('main', id='main')
    if not main_content:
        print("ğŸš¨ RSS Gen: <main id='main'> ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return []

    article_list_ul = main_content.find('ul', class_='list list-thumbnail-horizontal')
    if not article_list_ul:
        article_list_ul = main_content.find('ul', class_=lambda c: c and 'list' in c and 'list-thumbnail-horizontal' in c)
        if not article_list_ul:
             print("ğŸš« RSS Gen: <main id='main'> ë‚´ì—ì„œ ê¸°ì‚¬ ëª©ë¡ ul íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
             return []

    list_items = article_list_ul.find_all('li', class_='list-item', recursive=False)
    if not list_items:
        print("ğŸš« RSS Gen: ê¸°ì‚¬ ëª©ë¡ ul íƒœê·¸ ë‚´ì—ì„œ <li class='list-item'>ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    korea_tz = datetime.timezone(datetime.timedelta(hours=9)) # KST (UTC+9)

    for item_li in list_items:
        anchor_tag = item_li.find('a', class_='item-inner')
        if not anchor_tag: continue

        article_url = anchor_tag.get('href')
        if not article_url: continue
            
        text_div = anchor_tag.find('div', class_='item-text')
        if not text_div: continue

        title = "ì œëª© ì—†ìŒ"
        summary = "ìš”ì•½ ì—†ìŒ"
        author_name = "ì‘ì„±ì ë¯¸ìƒ"
        published_time_obj = datetime.datetime.now(datetime.timezone.utc) # ê¸°ë³¸ê°’

        title_div = text_div.find('div', class_='item-title')
        if title_div:
            title = title_div.text.strip()

        excerpt_div = text_div.find('div', class_='item-excerpt')
        if excerpt_div:
            if excerpt_div.p:
                summary = " ".join(excerpt_div.p.text.split())
            else:
                summary = " ".join(excerpt_div.text.split())
        
        subinfo_tags = text_div.find_all('div', class_='item-subinfo')
        
        if len(subinfo_tags) > 0:
            author_candidate = subinfo_tags[0].text.strip()
            if author_candidate:
                author_name = author_candidate

        if len(subinfo_tags) > 1:
            date_str_candidate = subinfo_tags[1].text.strip()
            if date_str_candidate:
                try:
                    dt_naive = datetime.datetime.strptime(date_str_candidate, "%Y.%m.%d")
                    published_time_obj = dt_naive.replace(tzinfo=korea_tz)
                except ValueError:
                    print(f"âš ï¸ RSS Gen: ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì˜¤ë¥˜ (ê¸°ì‚¬: '{title[:30]}...'): '{date_str_candidate}'.")
        
        articles_data.append({
            'title': title,
            'author': author_name,
            'summary': summary,
            'url': article_url,
            'published_time': published_time_obj
        })
    return articles_data

def create_feed_xml_for_rss(articles_data, site_url):
    fg = FeedGenerator()
    fg.title('Neosherlock Archives RSS Feed (ë™ì  ìƒì„±)')
    fg.link(href=site_url, rel='alternate')
    fg.description('neosherlock.com ì•„ì¹´ì´ë¸Œì˜ ê¸°ì‚¬ ì •ë³´ RSS í”¼ë“œ (Streamlit ì•±ì—ì„œ ë™ì  ìƒì„±)')
    fg.language('ko')
    
    for article_info in articles_data:
        fe = fg.add_entry()
        fe.id(article_info['url']) 
        fe.title(article_info['title'])
        fe.link(href=article_info['url'], rel='alternate')
        fe.description(article_info['summary'])
        fe.author({'name': article_info['author']})
        fe.pubDate(article_info['published_time'])
    
    return fg.rss_str(pretty=True)

def save_rss_to_file(rss_xml_bytes, filepath):
    try:
        with open(filepath, 'wb') as f:
            f.write(rss_xml_bytes)
        return True
    except IOError as e:
        print(f"ğŸš« RSS Gen: íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {filepath} - {e}")
        return False

@st.cache_data(ttl=3600) # 1ì‹œê°„ (3600ì´ˆ) ë™ì•ˆ ê²°ê³¼ ìºì‹œ
def generate_and_save_rss_feed_cached(output_filepath):
    """
    RSS í”¼ë“œë¥¼ ìƒì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. Streamlit UI ì§ì ‘ í˜¸ì¶œì€ í”¼í•©ë‹ˆë‹¤.
    """
    print(f"[{datetime.datetime.now()}] RSS í”¼ë“œ ìƒì„± ì‹œë„: {output_filepath}") # ì„œë²„ ë¡œê·¸
    
    html_content = fetch_html_for_rss(RSS_ARCHIVES_URL)
    if not html_content:
        print("RSS Gen: HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨.")
        return False

    articles = parse_articles_for_rss(html_content)
    # ê¸°ì‚¬ê°€ ì—†ì–´ë„ ë¹ˆ í”¼ë“œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, articlesê°€ ë¹„ì—ˆë‹¤ê³  ë°”ë¡œ Falseë¥¼ ë°˜í™˜í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.
    # print(f"RSS Gen: íŒŒì‹±ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}")

    rss_xml_bytes = create_feed_xml_for_rss(articles, RSS_ARCHIVES_URL)
    
    if save_rss_to_file(rss_xml_bytes, output_filepath):
        print(f"RSS Gen: '{output_filepath}' íŒŒì¼ ì €ì¥ ì„±ê³µ.")
        return True
    else:
        print(f"RSS Gen: '{output_filepath}' íŒŒì¼ ì €ì¥ ì‹¤íŒ¨.")
        return False

# --- RSS Feed Display Functions (ì´ì „ê³¼ ìœ ì‚¬) ---
def load_feed_for_display(file_path):
    """í‘œì‹œë¥¼ ìœ„í•´ ë¡œì»¬ RSS íŒŒì¼ì„ ë¡œë“œí•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤."""
    if not os.path.exists(file_path):
        # ì´ í•¨ìˆ˜ëŠ” RSS ìƒì„± í›„ í˜¸ì¶œë˜ë¯€ë¡œ, íŒŒì¼ì´ ì—†ë‹¤ë©´ ìƒì„± ì‹¤íŒ¨ë¥¼ ì˜ë¯¸í•  ìˆ˜ ìˆìŒ
        # st.errorëŠ” í˜¸ì¶œí•˜ëŠ” ìª½ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ ë©”ì‹œì§€ ë°˜í™˜ ì•ˆí•¨
        return None
    try:
        feed = feedparser.parse(file_path)
        if feed.bozo:
            # UIì— ì§ì ‘ ê²½ê³ í•˜ê¸°ë³´ë‹¤ í˜¸ì¶œí•œ ìª½ì—ì„œ ì²˜ë¦¬
            print(f"âš ï¸ RSS Display: í”¼ë“œ íŒŒì‹± ì¤‘ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥ì„±: {feed.bozo_exception}")
        return feed
    except Exception as e:
        print(f"ğŸš¨ RSS Display: íŒŒì¼ ì½ê¸°/íŒŒì‹± ì¤‘ ì˜ˆì™¸: {e}")
        return None

# --- Main Application UI ---
st.title("ğŸ“° ì…œë¡ ì•„í‹°í´ í”¼ë“œ (ìë™ ì—…ë°ì´íŠ¸)")
st.caption(f"'{RSS_FILE_PATH}' íŒŒì¼ì„ ì‚¬ìš©í•˜ë©°, ì£¼ê¸°ì ìœ¼ë¡œ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
st.markdown("---")

# RSS í”¼ë“œ ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸ ì‹œë„ (ìºì‹œëœ í•¨ìˆ˜ í˜¸ì¶œ)
with st.spinner("ìµœì‹  ê¸°ì‚¬ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  RSS í”¼ë“œë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”."):
    generation_successful = generate_and_save_rss_feed_cached(RSS_FILE_PATH)

if generation_successful:
    st.success("RSS í”¼ë“œê°€ ìµœì‹  ìƒíƒœë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    st.warning(
        "ìµœì‹  RSS í”¼ë“œë¥¼ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
        f"ì´ì „ì— ì €ì¥ëœ '{RSS_FILE_PATH}' íŒŒì¼ì´ ìˆë‹¤ë©´ í•´ë‹¹ ë‚´ìš©ì„ í‘œì‹œí•©ë‹ˆë‹¤."
    )

# ìƒì„±ë˜ì—ˆê±°ë‚˜ ê¸°ì¡´ì— ìˆë˜ RSS íŒŒì¼ ë¡œë“œ ì‹œë„
feed_data = load_feed_for_display(RSS_FILE_PATH)

if feed_data:
    if feed_data.entries:
        sorted_entries = sorted(
            feed_data.entries,
            key=lambda entry: entry.published_parsed if hasattr(entry, 'published_parsed') else (0,0,0,0,0,0,0,0,0),
            reverse=True
        )
        
        for entry in sorted_entries:
            with st.container():
                if hasattr(entry, 'title') and hasattr(entry, 'link'):
                    st.markdown(
                        f"""
                        <a href="{entry.link}" target="_blank" style="text-decoration: none; color: inherit;">
                            <h3>{entry.title}</h3>
                        </a>
                        """,
                        unsafe_allow_html=True
                    )
                elif hasattr(entry, 'title'):
                    st.markdown(f"<h3>{entry.title}</h3>", unsafe_allow_html=True)

                meta_info_parts = []
                if hasattr(entry, 'author') and entry.author:
                    meta_info_parts.append(f"ğŸ‘¤ {entry.author}")

                if hasattr(entry, 'published_parsed') and entry.published_parsed:
                    try:
                        utc_dt = datetime.datetime(*entry.published_parsed[:6], tzinfo=datetime.timezone.utc)
                        kst_tz = datetime.timezone(datetime.timedelta(hours=9))
                        kst_dt = utc_dt.astimezone(kst_tz)
                        meta_info_parts.append(f"ğŸ“… {kst_dt.strftime('%Yë…„ %mì›” %dì¼ %H:%M KST')}")
                    except Exception:
                        if hasattr(entry, 'published'):
                             meta_info_parts.append(f"ğŸ“… {entry.published}")
                elif hasattr(entry, 'published') and entry.published:
                    meta_info_parts.append(f"ğŸ“… {entry.published}")

                if meta_info_parts:
                    st.caption("  Â·  ".join(meta_info_parts))

                if hasattr(entry, 'summary') and entry.summary:
                    with st.expander("ìš”ì•½ ë³´ê¸°", expanded=False):
                        st.markdown(entry.summary, unsafe_allow_html=True)
                
                st.markdown("---")
    elif feed_data.bozo:
         st.warning("í”¼ë“œë¥¼ íŒŒì‹±í–ˆìœ¼ë‚˜, í‘œì‹œí•  ê²Œì‹œê¸€ì´ ì—†ê±°ë‚˜ í”¼ë“œ ë°ì´í„°ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        st.info("â„¹ï¸ í”¼ë“œì— í‘œì‹œí•  ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
elif not os.path.exists(RSS_FILE_PATH): # ìƒì„±ë„ ì‹¤íŒ¨í–ˆê³ , ê¸°ì¡´ íŒŒì¼ë„ ì—†ëŠ” ê²½ìš°
    st.error(
        f"'{RSS_FILE_PATH}' íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šê³ , ìƒˆë¡œ ìƒì„±í•˜ëŠ” ë°ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
        "ê¸°ì‚¬ í”¼ë“œë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
else: # íŒŒì¼ì€ ì¡´ì¬í•˜ë‚˜ load_feed_for_displayì—ì„œ None ë°˜í™˜ (ì‹¬ê°í•œ íŒŒì‹± ì˜¤ë¥˜ ë“±)
    st.error(f"'{RSS_FILE_PATH}' íŒŒì¼ì„ ì½ê±°ë‚˜ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


st.markdown("---")
st.markdown("<p style='text-align: center; color: grey; font-size: small;'>Neosherlock Article Feed | Powered by Streamlit</p>", unsafe_allow_html=True)