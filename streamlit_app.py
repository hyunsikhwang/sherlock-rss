# app.py

import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import xml.etree.ElementTree as ET
from datetime import datetime
from email.utils import format_datetime

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 1) ì•„ì¹´ì´ë¸Œ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def fetch_archive_urls(archive_url):
    resp = requests.get(archive_url)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')
    
    article_links = set()
    for link in soup.find_all('a', href=True):
        href = link['href']
        # https://www.neosherlock.com/archives/ìˆ«ì í˜•íƒœë§Œ ê³¨ë¼ë‚´ê¸°
        if re.match(r'https?://www\.neosherlock\.com/archives/\d+$', href):
            article_links.add(href)
    return list(article_links)


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 2) ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì œëª©, ê¸°ìëª…, ë³¸ë¬¸, ê²Œì‹œì¼, ë§í¬ ì¶”ì¶œ
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def parse_article(article_url):
    resp = requests.get(article_url)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, 'html.parser')
    
    # 2-1) ì œëª© (<h1> íƒœê·¸)
    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else 'No Title'
    
    # 2-2) ê¸°ìëª…(meta ì†ì„± ë˜ëŠ” í´ë˜ìŠ¤ íƒìƒ‰)
    author = 'Unknown'
    author_meta = soup.find('meta', {'property': 'article:author'})
    if author_meta and author_meta.get('content'):
        author = author_meta['content']
    else:
        author_tag = soup.find(class_=re.compile(r'author|byline', re.I))
        if author_tag:
            author = author_tag.get_text(strip=True)
    
    # 2-3) ê²Œì‹œì¼(meta ì†ì„±)
    pub_date = None
    date_meta = soup.find('meta', {'property': 'article:published_time'})
    if date_meta and date_meta.get('content'):
        try:
            dt = datetime.fromisoformat(date_meta['content'].replace('Z', '+00:00'))
            pub_date = format_datetime(dt)  # RFC2822 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        except Exception:
            pub_date = None
    
    # 2-4) ë³¸ë¬¸(ë‚´ìš©) ì¶”ì¶œ
    content = ''
    content_div = soup.find('div', class_=re.compile(r'entry-content|post-content', re.I))
    if content_div:
        content = content_div.get_text(separator='\n', strip=True)
    else:
        # fallback: ëª¨ë“  <p> íƒœê·¸ë¥¼ í•©ì³ì„œ ë³¸ë¬¸ìœ¼ë¡œ ì‚¬ìš©
        paragraphs = soup.find_all('p')
        content = '\n'.join(p.get_text(strip=True) for p in paragraphs)
    
    return {
        'title': title,
        'author': author,
        'pub_date': pub_date,
        'content': content,
        'link': article_url
    }


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 3) RSS XML ìƒì„± í•¨ìˆ˜
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def generate_rss(feeds, site_title, site_link, site_description):
    rss = ET.Element('rss', version='2.0')
    channel = ET.SubElement(rss, 'channel')
    
    ET.SubElement(channel, 'title').text = site_title
    ET.SubElement(channel, 'link').text = site_link
    ET.SubElement(channel, 'description').text = site_description
    ET.SubElement(channel, 'lastBuildDate').text = format_datetime(datetime.utcnow())
    
    for feed in feeds:
        item = ET.SubElement(channel, 'item')
        ET.SubElement(item, 'title').text = feed['title']
        ET.SubElement(item, 'link').text = feed['link']
        ET.SubElement(item, 'description').text = feed['content']
        if feed['author']:
            ET.SubElement(item, 'author').text = feed['author']
        if feed['pub_date']:
            ET.SubElement(item, 'pubDate').text = feed['pub_date']
    
    return ET.tostring(rss, encoding='utf-8', xml_declaration=True)


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 4) RSS íŒŒì¼ ìƒì„± ë° feeds ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def generate_and_save_rss():
    archive_url = 'https://www.neosherlock.com/archives'
    
    # 4-1) ì•„ì¹´ì´ë¸Œ URL ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    article_urls = fetch_archive_urls(archive_url)
    
    # 4-2) ê° ê¸°ì‚¬ íŒŒì‹±
    feeds = []
    for url in article_urls:
        try:
            article_data = parse_article(url)
            feeds.append(article_data)
        except Exception:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê±´ë„ˆëœ€
            continue
    
    # 4-3) RSS XML ìƒì„±
    rss_xml = generate_rss(
        feeds,
        site_title='ì…œë¡ Archives',
        site_link=archive_url,
        site_description='ë„¤ì˜¤ì…œë¡ ì•„ì¹´ì´ë¸Œ RSS í”¼ë“œ'
    )
    
    # 4-4) íŒŒì¼ë¡œ ì €ì¥
    file_path = 'neosherlock_rss.xml'
    with open(file_path, 'wb') as f:
        f.write(rss_xml)
    
    return feeds


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# 5) RSS XMLì„ íŒŒì‹±í•´ì„œ í™”ë©´ ì¶œë ¥ìš© ë¦¬ìŠ¤íŠ¸ ìƒì„±
# (ì´ì „ ë‹¨ê³„ì˜ feedsë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ë˜ì§€ë§Œ,
#   XML ì €ì¥ í™•ì¸ ì°¨ì›ì—ì„œ ë‹¤ì‹œ íŒŒì‹±í•˜ëŠ” êµ¬ì¡°ë„ ê°€ëŠ¥)
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def load_feeds_from_file(path):
    tree = ET.parse(path)
    root = tree.getroot()
    channel = root.find('channel')
    items = channel.findall('item')
    
    feeds = []
    for item in items:
        title = item.find('title').text or "No Title"
        link = item.find('link').text or ""
        description = item.find('description').text or ""
        author = item.find('author').text if item.find('author') is not None else ""
        pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ""
        
        feeds.append({
            'title': title,
            'link': link,
            'description': description,
            'author': author,
            'pub_date': pub_date
        })
    return feeds


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# Streamlit UI
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def main():
    st.set_page_config(page_title="ì…œë¡ Archive Feed", layout="centered")
    st.title("ğŸ” ì…œë¡ ì•„ì¹´ì´ë¸Œ ì‹¤ì‹œê°„ Feed")
    st.write("ì•± ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ìµœì‹  ì•„ì¹´ì´ë¸Œë¥¼ í¬ë¡¤ë§í•˜ì—¬ RSSë¥¼ ìƒì„±í•˜ê³ , ê°„ê²°í•œ ë””ìì¸ìœ¼ë¡œ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.")
    
    # 5-1) RSS íŒŒì¼ ìƒì„±
    with st.spinner("ë„¤ì˜¤ì…œë¡ ì•„ì¹´ì´ë¸Œì—ì„œ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ RSS íŒŒì¼ ìƒì„± ì¤‘..."):
        try:
            feeds = generate_and_save_rss()
            file_path = 'neosherlock_rss.xml'
        except Exception as e:
            st.error(f"RSS ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")
            return
    
    # 5-2) ìƒì„±ëœ XMLì„ ë‹¤ì‹œ íŒŒì‹±í•˜ì—¬ í™”ë©´ ì¶œë ¥ìš© ë¦¬ìŠ¤íŠ¸ í™•ë³´
    try:
        display_feeds = load_feeds_from_file(file_path)
    except Exception as e:
        st.error(f"ìƒì„±ëœ RSS XML íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{e}")
        return
    
    st.success("âœ… RSS íŒŒì¼ ìƒì„± ë° íŒŒì‹± ì™„ë£Œ!")
    st.markdown("---")
    
    # 5-3) ê° ê¸°ì‚¬(item)ë¥¼ í™”ë©´ì— ì¶œë ¥
    for feed in display_feeds:
        st.markdown(f"## [{feed['title']}]({feed['link']})")
        
        # ê¸°ìëª… & ê²Œì‹œì¼
        meta_parts = []
        if feed['author']:
            meta_parts.append(f"âœï¸ **ê¸°ì**: {feed['author']}")
        if feed['pub_date']:
            meta_parts.append(f"ğŸ—“ï¸ **ê²Œì‹œì¼**: {feed['pub_date']}")
        if meta_parts:
            st.markdown("  ".join(meta_parts))
        
        # ë³¸ë¬¸ ìŠ¤ë‹ˆí« (300ìê¹Œì§€)
        snippet = feed['description'].strip()
        if len(snippet) > 300:
            snippet = snippet[:300] + "..."
        st.write(snippet)
        
        st.markdown("---")


if __name__ == "__main__":
    main()
